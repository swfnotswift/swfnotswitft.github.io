<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>langchain · Hexo</title><meta name="description" content="## Start
让踏入Langchain的学习之旅，时间很短，让我们现在就开始吧！
首先第一步，安装好Langchain相关的依赖。
这一步教程可以参考官方网站
然后，让我们定义一个模型 像下面这样 1234567llm=ChatOpenAI(      model=&quot;xxxxxx&quot;,  "><meta name="og:description" content="## Start
让踏入Langchain的学习之旅，时间很短，让我们现在就开始吧！
首先第一步，安装好Langchain相关的依赖。
这一步教程可以参考官方网站
然后，让我们定义一个模型 像下面这样 1234567llm=ChatOpenAI(      model=&quot;xxxxxx&quot;,  "><meta name="twitter:site" content="Hexo"><meta name="twitter:title" content="langchain"><meta name="twitter:card" content="summary"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 7.1.1"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="container" id="stage"><div class="row"><div class="col-sm-3 col-xs-12 side-container invisible" id="side-bar"><div class="vertical-text site-title"><h3 class="site-title-small" tabindex="-1"><a class="a-title" href="/">From Swift</a></h3><h1 class="site-title-large" tabindex="-1"><a class="a-title" href="/">博客而已</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div class="site-title-links" id="site-nav"><ul><li><a href="/">Home</a></li><li><a href="/archives">Archive</a></li><li><a href="/tags">Tags</a></li><li><a href="/about/index.html">about</a></li><li class="soc"><a href="https://github.com/swfnotswift" target="_blank" rel="noopener noreferrer" aria-label="Github"><i class="fa fa-github">&nbsp;</i></a><a href="http://example.com/atom.xml" target="_blank" rel="noopener noreferrer" aria-label="RSS"><i class="fa fa-rss">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2024&nbsp;<a target="_blank" href="http://example.com" rel="noopener noreferrer">John Doe</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div class="col-sm-9 col-xs-12 main-container invisible" id="main-container"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>langchain</a></p><p class="post-meta"><span class="date meta-item">Posted at&nbsp;2024-05-31</span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a class="a-tag" href="/tags/langchain学习/" title="langchain学习">langchain学习</a><span>&nbsp;</span></span></p><p class="post-abstract"><h2 id="start">## Start</h2>
<p>让踏入Langchain的学习之旅，时间很短，让我们现在就开始吧！</p>
<p>首先第一步，安装好Langchain相关的依赖。
这一步教程可以参考官方网站</p>
<p>然后，让我们定义一个<strong>模型</strong> 像下面这样 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">llm=ChatOpenAI(  </span><br><span class="line">    model=<span class="string">"xxxxxx"</span>,  </span><br><span class="line">    base_url=<span class="string">"xxxxxxx"</span>,  </span><br><span class="line">    api_key=<span class="string">"xxxxx"</span>,  </span><br><span class="line">    temperature=<span class="number">0.3</span>  </span><br><span class="line">)</span><br></pre></td></tr></table></figure>
请注意，来自模型的响应是一个 .NET
文件，其中包含一个字符串响应以及有关响应的其他元数据。很多时候，我们可能只想处理字符串响应。我们可以使用一个简单的<strong>输出解析器</strong>，只解析出这个响应
。 我们导入一个简单的解释器 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parser=StrOutputParser()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
很好，现在我们已经定义了模型和解释器了 让我们开始操作吧！
<strong><em>方法一:</em></strong> parse和llm分开： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result=llm.invoke(<span class="string">"hello"</span>)  </span><br><span class="line">parser.invoke(result)  </span><br><span class="line"><span class="built_in">print</span>(result)  </span><br><span class="line"><span class="built_in">print</span>(parser.invoke(result))</span><br></pre></td></tr></table></figure>
输出结果如下： <img src="/2024/05/31/langchain/结果1.png">
从输出结果中我们可以看出，如果没有通过解释器，result中会包括除了字符串响应以外的其他东西
<strong><em>方法二：</em></strong></p>
<p>我们可以将模型与输出解析器 "链
"起来。用模型和解释器绑定在一起，链接起来
，这意味着在这个链中，每次都会调用这个输出解析器。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">chain=llm|parser  </span><br><span class="line"><span class="built_in">print</span>(chain.invoke(<span class="string">"how do you think about BYD"</span>))  </span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure> As a
large language model, I don't have personal opinions or emotions.
However, I can provide you with factual information about BYD (Build
Your Dreams), which is a Chinese multinational company primarily known
for its electric vehicles (EVs) and energy storage solutions. BYD is one
of the leading players in the global automotive industry, particularly
in the electric vehicle market. They have been successful in developing
affordable and eco-friendly vehicles, and their growth and innovation
have been significant. The company is also known for its battery
technology, which is a key advantage in the EV sector. Overall, BYD is
often viewed positively for its contributions to the transition to
sustainable mobility.</p>
<p>这样再这个链中，我们每次都调用了parse
<strong><em>方法三：</em></strong> 我们可以创建提示模板 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">system_template = <span class="string">"Translate the following into {language}:"</span>  </span><br><span class="line">prompt_template = ChatPromptTemplate.from_messages([(<span class="string">"system"</span>, system_template),(<span class="string">"user"</span>, <span class="string">"{text}"</span>)])  </span><br><span class="line">result=prompt_template.invoke({<span class="string">"language"</span>:<span class="string">"Chinese"</span>,<span class="string">"text"</span>:<span class="string">"hello"</span>})  </span><br><span class="line"><span class="built_in">print</span>(result.to_messages())</span><br><span class="line"><span class="comment">#[SystemMessage(content='Translate the following into Chinese:'), HumanMessage(content='hello')]</span></span><br><span class="line">chain=prompt_template|llm|parser  </span><br><span class="line"><span class="built_in">print</span>(chain.invoke({<span class="string">"language"</span>: <span class="string">"Chinese"</span>, <span class="string">"text"</span>: <span class="string">"Ciallo"</span>}))</span><br><span class="line"><span class="comment">#Ciallo是一个意大利名字，直译可能没有具体含义，但在某些方言中，它可能类似于英语中的"hi"或"alright"，是一种非正式的问候或打招呼用语。中文可以翻译为"嗨"或"你好"，但在没有更多上下文的情况下，一般译为"嗨"。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
从上段代码可以看出 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result.to_messages()</span><br></pre></td></tr></table></figure>
可以输出模板的相关信息，将其合成Chain输出后，可以根据提示模板输出相关结果。
## server开发 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langserve <span class="keyword">import</span> add_routes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create prompt template</span></span><br><span class="line">system_template = <span class="string">"Translate the following into {language}:"</span></span><br><span class="line">prompt_template = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">'system'</span>, system_template),</span><br><span class="line">    (<span class="string">'user'</span>, <span class="string">'{text}'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create model</span></span><br><span class="line">model = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create parser</span></span><br><span class="line">parser = StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create chain</span></span><br><span class="line">chain = prompt_template | model | parser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. App definition</span></span><br><span class="line">app = FastAPI(</span><br><span class="line">  title=<span class="string">"LangChain Server"</span>,</span><br><span class="line">  version=<span class="string">"1.0"</span>,</span><br><span class="line">  description=<span class="string">"A simple API server using LangChain's Runnable interfaces"</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Adding chain route</span></span><br><span class="line"></span><br><span class="line">add_routes(</span><br><span class="line">    app,</span><br><span class="line">    chain,</span><br><span class="line">    path=<span class="string">"/chain"</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line"></span><br><span class="line">    uvicorn.run(app, host=<span class="string">"localhost"</span>, port=<span class="number">8000</span>)</span><br></pre></td></tr></table></figure> 主要分为四个步骤</p>
<h2 id="build-a-chatbot">## Build a Chatbot</h2>
<p>在这一节，我们会举例说明如何设计LLM驱动的chatbot</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model.invoke([HumanMessage(content=<span class="string">"Hi! I'm Bob"</span>)])</span><br><span class="line"><span class="comment"># content='Hello Bob! Nice to meet you. How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 25, 'total_tokens': 41}, 'model_name': 'qwen1.5-14b-chat', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-861b0361-3414-4420-8d99-c3ae75ec6e26-0' usage_metadata={'input_tokens': 25, 'output_tokens': 16, 'total_tokens': 41}</span></span><br><span class="line"></span><br><span class="line">result=model.invoke([HumanMessage(content=<span class="string">"What's my name?"</span>)])</span><br><span class="line"><span class="comment">#content="I'm sorry, but I don't have the ability to know your name as you haven't provided it or context for me to remember it. If you're asking for a nickname or a common name, I can't assist with that. If you'd like, you can tell me your name, and I'll be happy to address you by it." response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 24, 'total_tokens': 96}, 'model_name': 'qwen1.5-14b-chat', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-49d9af18-1dbd-42af-a173-646abddc78c9-0' usage_metadata={'input_tokens': 24, 'output_tokens': 72, 'total_tokens': 96}</span></span><br><span class="line"></span><br><span class="line">result2=model.invoke([  </span><br><span class="line">    HumanMessage(content=<span class="string">"Hi！I‘m Bob?"</span>),  </span><br><span class="line">    AIMessage(content=<span class="string">"Hello Bob! How can I assist you today?"</span>),  </span><br><span class="line">    HumanMessage(content=<span class="string">"What's my name?"</span>),  </span><br><span class="line">])</span><br><span class="line"><span class="comment">#content='Your name is Bob.' response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 51, 'total_tokens': 57}, 'model_name': 'qwen1.5-14b-chat', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-5f34d94f-0f57-48d9-bb1a-054797057507-0' usage_metadata={'input_tokens': 51, 'output_tokens': 6, 'total_tokens': 57}</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如上图所示，我们可以看到model的回答没有记忆性
如果我们希望model能够有记忆性我们需要将回复传递进去</p>
<h2 id="解决方法">解决方法</h2>
<p>我们可以使用 "消息历史记录
"类来封装我们的模型，使其有状态。这将跟踪模型的输入和输出，并将它们存储在某个数据存储中。未来的交互将加载这些消息，并将它们作为输入的一部分传递到链中。让我们看看如何使用它！
首先我们安装langchain-community
<code>pip install langchain_community</code></p>
<p>之后，我们就可以导入相关类，并设置我们的链，该链将包裹模型并添加此消息历史记录。</p>
<p>这里的关键部分是我们作为 <strong>get_session_history</strong>
传入的函数。该函数将接收一个 <strong>session_id</strong>
并返回一个消息历史对象。该 <strong>session_id</strong>
用于区分不同的会话，在调用新链时应作为配置的一部分传入（我们将演示如何做到这一点）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.chat_history <span class="keyword">import</span> BaseChatMessageHistory  </span><br><span class="line"><span class="keyword">from</span> langchain_community.chat_message_histories <span class="keyword">import</span> ChatMessageHistory  </span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables.history <span class="keyword">import</span> RunnableWithMessageHistory  </span><br><span class="line">  </span><br><span class="line">store={}  </span><br><span class="line">model=ChatOpenAI(  </span><br><span class="line">    model=<span class="string">"xxx"</span>,  </span><br><span class="line">    base_url=<span class="string">"xxxx"</span>,  </span><br><span class="line">    api_key=<span class="string">"xxxxxx"</span>,  </span><br><span class="line">    temperature=<span class="number">0.3</span>  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_session_history</span>(<span class="params">session_id:<span class="built_in">str</span></span>)-&gt;BaseChatMessageHistory:  </span><br><span class="line">    <span class="keyword">if</span> session_id <span class="keyword">not</span> <span class="keyword">in</span> store:  </span><br><span class="line">        store[session_id]=ChatMessageHistory()  </span><br><span class="line">    <span class="keyword">return</span> store[session_id]  </span><br><span class="line">with_message_history = RunnableWithMessageHistory(model,get_session_history)</span><br></pre></td></tr></table></figure>
<p>现在，我们需要创建一个<strong>config</strong>，每次都将其传递给
runnable。该配置包含的信息不是直接输入的一部分，但仍然有用。在本例中，我们希望包含一个
session_id。这个<strong>config</strong>应该如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">config = {<span class="string">"configurable"</span>: {<span class="string">"session_id"</span>: <span class="string">"abc2"</span>}}</span><br><span class="line"></span><br><span class="line">response = with_message_history.invoke(  </span><br><span class="line">[HumanMessage(content=<span class="string">"Hi! I'm Bob"</span>)],  </span><br><span class="line">config=config,  </span><br><span class="line">)</span><br><span class="line">response.content</span><br><span class="line"></span><br><span class="line">response = with_message_history.invoke(  </span><br><span class="line">[HumanMessage(content=<span class="string">"What's my name?"</span>)],  </span><br><span class="line">config=config,  </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response.content</span><br><span class="line"><span class="comment">#Hello Bob! Nice to meet you. How can I assist you today? If you have any questions, need information, or just want to chat, feel free to ask.</span></span><br><span class="line">Your name <span class="keyword">is</span> Bob.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model=ChatOpenAI(  </span><br><span class="line">    model=<span class="string">"qwen1.5-14b-chat"</span>,  </span><br><span class="line">    base_url=<span class="string">"http://124.70.213.108:7009/v1"</span>,  </span><br><span class="line">    api_key=<span class="string">"EMPTY"</span>,  </span><br><span class="line">    temperature=<span class="number">0.3</span>  </span><br><span class="line">)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_session_history</span>(<span class="params">session_id:<span class="built_in">str</span></span>)-&gt;BaseChatMessageHistory:  </span><br><span class="line">    <span class="keyword">if</span> session_id <span class="keyword">not</span>  <span class="keyword">in</span> store:  </span><br><span class="line">        store[session_id]=ChatMessageHistory()  </span><br><span class="line">    <span class="keyword">return</span> store[session_id]  </span><br><span class="line">with_message_history = RunnableWithMessageHistory(model,get_session_history)  </span><br><span class="line">  </span><br><span class="line">config = {<span class="string">"configurable"</span>: {<span class="string">"session_id"</span>: <span class="string">"abc2"</span>}}  </span><br><span class="line">  </span><br><span class="line">response = with_message_history.invoke(  </span><br><span class="line">[HumanMessage(content=<span class="string">"Hi! I'm Bob"</span>)],  </span><br><span class="line">config=config,  </span><br><span class="line">)  </span><br><span class="line">c1=response.content  </span><br><span class="line"><span class="built_in">print</span>(c1)  </span><br><span class="line"><span class="comment">#Hello Bob! Nice to meet you. How can I assist you today? If you have any questions, need information, or just want to chat, feel free to ask.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">response = with_message_history.invoke(  </span><br><span class="line">[HumanMessage(content=<span class="string">"What's my name?"</span>)],  </span><br><span class="line">config=config,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line">c2=response.content  </span><br><span class="line"><span class="built_in">print</span>(c2)  </span><br><span class="line"><span class="comment">#Your name is Bob.</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">config2 = {<span class="string">"configurable"</span>: {<span class="string">"session_id"</span>: <span class="string">"abc3"</span>}}  </span><br><span class="line">  </span><br><span class="line">response = with_message_history.invoke(  </span><br><span class="line">    [HumanMessage(content=<span class="string">"What's my name?"</span>)],  </span><br><span class="line">    config=config2,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(response.content)  </span><br><span class="line"><span class="comment">#I'm sorry, but I don't have that information. You haven't provided your name, and as an AI, I don't have access to personal data without your input. If you'd like, you can tell me your name, and I'll remember it.</span></span><br><span class="line"></span><br><span class="line">response = with_message_history.invoke(  </span><br><span class="line">    [HumanMessage(content=<span class="string">"What's my name?"</span>)],  </span><br><span class="line">    config=config,  </span><br><span class="line">)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br><span class="line"><span class="comment">#Your name is still Bob. It seems like you're asking the same question. If you have a different name or need confirmation, please let me know.</span></span><br></pre></td></tr></table></figure>
<p>如上图所示，我们使用相同的config可以使得对话连续，而不同的config可以看作是不同的对象。</p>
<h2 id="prompt-templates">Prompt templates</h2>
<p>提示模板有助于将原始用户信息转化为 LLM
可以使用的格式。在本例中，原始的用户输入只是一条信息，我们要将其传递给
LLM。现在让我们把它变得复杂一些。首先，让我们在系统消息中添加一些自定义指令（但仍以消息作为输入）。接下来，除了信息外，我们还要添加更多输入信息。</p>
<p>首先，让我们添加一条系统消息。为此，我们将创建一个
<strong>ChatPromptTemplate</strong>。我们将利用<strong>MessagesPlaceholder</strong>
来传递所有信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (</span><br><span class="line">            <span class="string">"system"</span>,</span><br><span class="line">            <span class="string">"You are a helpful assistant. Answer all questions to the best of your ability."</span>,</span><br><span class="line">        ),</span><br><span class="line">        MessagesPlaceholder(variable_name=<span class="string">"messages"</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chain = prompt | model</span><br></pre></td></tr></table></figure>
<pre><code>在对话模型(chat model)中， prompt主要是封装在Message中，LangChain提供了一些MessagePromptTemplate，方便我们直接使用Message生成prompt。
MessagesPlaceholder:该提示模板负责在特定位置添加信息列表。在上面的 ChatPromptTemplate 中，我们看到了如何格式化两条信息，每条信息都是一个字符串。但如果我们想让用户传入一个信息列表，并将其插入特定位置呢？这就是使用 MessagesPlaceholder 的方法。</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">response = chain.invoke({<span class="string">"messages"</span>: [HumanMessage(content=<span class="string">"hi! I'm swf"</span>)]})</span><br><span class="line">response.content</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</p></div><div class="share"><span>Share</span>&nbsp;<span class="soc"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></span><span class="soc"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></span><span class="soc"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=http://example.com/2024/05/31/langchain/%20Hexo%20langchain"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/2024/06/07/SAD%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" title="SAD期末复习"><i class="fa fa-angle-double-left"></i>&nbsp;Previous post: SAD期末复习</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/2024/04/06/docker%E5%AD%A6%E4%B9%A0/" title="docker学习">Next post: docker学习&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div><div id="lv-container" data-id="city" data-uid="MTAyMC81OTQ2Ny8zNTkyOQ=="><script type="text/javascript">(function (d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') {
        return;
    }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script><noscript> Please activate JavaScript for write a comment in LiveRe</noscript></div><div class="foreground background-text" style="color: #fff; padding: 10px 25px; width: 100%"><div style="vertical-align: middle;display: table-cell!important;"><div class="visible-lg visible-sm visible-md" style="vertical-align: middle;text-align:center;margin-right:15px"><img src="/images/pigeon.svg" width="150px" style="padding: 15px; vertical-align: middle;"><p style="font-size: 55%;vertical-align: middle;text-align:center;margin: 0">咕咕咕, 就快送到了</p></div></div><div style="padding: 15px 0;vertical-align: middle;display: table-cell;"><p style="font-size: 115%">哎呀，似乎评论系统在您的地区都无法正常工作。</p><p style="margin:0;">不过不要担心，来看看我们为您准备的备用方案 ——<br>        1. 将您的评论用信封装好<br>        2. 使用信鸽函至1476573945@qq.comexample.com<br>        3. 我们在收到您的评论后将立即审核并更新至网站<br><i><small>评论一经采用，信函恕不退还，信鸽也不退还，请知悉。</small></i></p></div></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2024&nbsp;<a target="_blank" href="http://example.com" rel="noopener noreferrer">John Doe</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>